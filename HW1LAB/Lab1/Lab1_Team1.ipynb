{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "Plase use <b>Python 3</b> in Jupyter Notebook.\n",
    "\n",
    "This lab focuses on classifcation and prediction. We will practice classification methods on a real world wetland mapping dataset. Each data sample contains several numeric features and a binary class label (0 for dry land, 1 for wetland). \n",
    "\n",
    "<b>Requirement</b>\n",
    "- <font color=red>Plese upload your Jupyter Notebook with required Data files and Python script files all in the SAME zipped FOLDER</font>\n",
    "\n",
    "- Please MAKE SURE your codes run smoothly without bugs in Jupyter Notebook with Python 3. \n",
    "    \n",
    "- <font color=red>Codes with bugs or errors that cannot run through in Jupyter notebook will be graded as ZERO for that part.</font> \n",
    "\n",
    "## Statement of Contribution\n",
    "\n",
    "We both tried the homework on our own. Ramey typed up programming solutions, and Lay reviewed them. Ramey worked on extra credit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier # Bagging Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.linear_model import LogisticRegression # Import Logistic Regression Classifier\n",
    "from sklearn.svm import SVC # Import SVM classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Please use the following codes to load the data. The training and test data are both saved in common separated values (CSV) format. The last column is class label. \n",
    "\n",
    "PLEASE RUN TEH CODES BELOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   near infra red   red  green  blue  class\n",
      "0              123  132    115   133      0\n",
      "1              152  150    119   187      1\n",
      "2              169  166    143   192      1\n",
      "3               55   49     43    97      0\n",
      "4              141  135    117   181      1\n",
      "   near infra red   red  green  blue  class\n",
      "0              137  140    129   150      0\n",
      "1              169  162    140   193      1\n",
      "2              124  110     89   162      1\n",
      "3              105  104     99   153      1\n",
      "4              105  102     88   173      1\n"
     ]
    }
   ],
   "source": [
    "#note: class 0 for dry land, class 1 for wetland\n",
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis='columns')\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis='columns')\n",
    "Y_test = test_dat['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Evaluate the performance of decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the codes below to train a decision tree, and make predictions on test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "dt = dt.fit(X_train,Y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the codes below to compute the \"Overall Accuracy\", as well as Precision, Recall, and F-score for the Wetland class (class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789\n",
      "Precision: 0.7445414847161572\n",
      "Recall: 0.7839080459770115\n",
      "F1-Score: 0.7637178051511757\n"
     ]
    }
   ],
   "source": [
    "# Fill in codes to calculate the values below; You can add any codes, but don't change the variable names\n",
    "\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_wet}\")\n",
    "print(f\"Recall: {recall_wet}\")\n",
    "print(f\"F1-Score: {F_wet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>ANSWER THE QUESTION BELOW.</font> How do you think about the performance shown by different metrics above. Is accuracy a good metric to reflect classification performance? Why? You can discuss your answer as a string. And print it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy score is 0.784\n",
      "\n",
      "The precision score is 0.7417\n",
      "\n",
      "The recall score is 0.7724\n",
      "\n",
      "The F1 score is 0.7567\n",
      "\n",
      "Although the accuracy is usually a misleading metric for model performance, in this case the precision\n",
      "shows that the out of all the predicted wetland, 74.17% of them are actually wetland. The recall score\n",
      "shows that out of all the actual wetland, 77.24% of them are predicted as wetland. The F1 score is the\n",
      "The F1 or F-Measure is the harmonic mean of Precision and Recall. This gives equal weight to precision\n",
      "and recall. The F1 score is 0.7567 which means that the accuracy is not a bad measure of model performance\n",
      "in this scenario.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MY ANSWER IS xxxxx\n",
    "ans = '''\n",
    "The accuracy score is 0.784\n",
    "\n",
    "The precision score is 0.7417\n",
    "\n",
    "The recall score is 0.7724\n",
    "\n",
    "The F1 score is 0.7567\n",
    "\n",
    "Although the accuracy is usually a misleading metric for model performance, in this case the precision\n",
    "shows that the out of all the predicted wetland, 74.17% of them are actually wetland. The recall score\n",
    "shows that out of all the actual wetland, 77.24% of them are predicted as wetland. The F1 score is the\n",
    "The F1 or F-Measure is the harmonic mean of Precision and Recall. This gives equal weight to precision\n",
    "and recall. The F1 score is 0.7567 which means that the accuracy is not a bad measure of model performance\n",
    "in this scenario.\n",
    "'''\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Overfitting issues\n",
    "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> Please re-train the decision tree model with a smaller number of training samples. <font color=red>DO NOT change the tree model parameters (e.g., minimum leaf node size) from Part 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   near infra red   red  green  blue  class\n",
      "0              143  150    145   156      0\n",
      "1              151  143    120   183      1\n",
      "2              100   98     86   156      0\n",
      "3              140  137    119   182      0\n",
      "4              147  147    138   160      0\n",
      "   near infra red   red  green  blue  class\n",
      "0              137  140    129   150      0\n",
      "1              169  162    140   193      1\n",
      "2              124  110     89   162      1\n",
      "3              105  104     99   153      1\n",
      "4              105  102     88   173      1\n",
      "Metrics of Model on Training Data\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "Metrics of Model on Test Data\n",
      "Accuracy: 0.716\n",
      "Precision: 0.6784869976359338\n",
      "Recall: 0.6597701149425287\n",
      "F1-Score: 0.6689976689976689\n"
     ]
    }
   ],
   "source": [
    "# Re-run the training and testing based on a smaller training set (smalltrain.csv).\n",
    "#note: class 0 for dry land, class 1 for wetland\n",
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"smalltrain.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis='columns')\n",
    "Y_train = train_dat['class'] \n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis='columns')\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "dt = dt.fit(X_train,Y_train)\n",
    "\n",
    "# Re-evaluate the trained model on test data, print out accuracy, precision, recall, F-score\n",
    "Y_pred = dt.predict(X_train)\n",
    "accuracy = metrics.accuracy_score(Y_train, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_train, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_train, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_train, Y_pred)\n",
    "\n",
    "print('Metrics of Model on Training Data')\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)\n",
    "\n",
    "Y_pred = dt.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print('Metrics of Model on Test Data')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_wet}\")\n",
    "print(f\"Recall: {recall_wet}\")\n",
    "print(f\"F1-Score: {F_wet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>ANSWER THE QUESTION BELOW.</font> What do you observe when the number of training samples decrese? Is this overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When the number of training samples decrease this is called overfitting because the model is more complex than the training \n",
      "data. This can be seen since the model has basically no training errors but is generalizing poorly to the test data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MY Answer: xxxxxxxxxxx\n",
    "ans = '''\n",
    "When the number of training samples decrease this is called overfitting because the model is more complex than the training \n",
    "data. This can be seen since the model has basically no training errors but is generalizing poorly to the test data.\n",
    "'''\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do another experiment to mitigate the effect of overfitting by decreasing the model complexity. We keep using the small training data (smalltrain.csv) that leads to overfitting. We decrease model complexity by having a larger minimum leaf node size (\"min_sample_split=30\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   near infra red   red  green  blue  class\n",
      "0              143  150    145   156      0\n",
      "1              151  143    120   183      1\n",
      "2              100   98     86   156      0\n",
      "3              140  137    119   182      0\n",
      "4              147  147    138   160      0\n",
      "   near infra red   red  green  blue  class\n",
      "0              137  140    129   150      0\n",
      "1              169  162    140   193      1\n",
      "2              124  110     89   162      1\n",
      "3              105  104     99   153      1\n",
      "4              105  102     88   173      1\n",
      "Metrics of Model on Test Data\n",
      "Accuracy: 0.757\n",
      "Precision: 0.6882352941176471\n",
      "Recall: 0.8068965517241379\n",
      "F1-Score: 0.742857142857143\n"
     ]
    }
   ],
   "source": [
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"smalltrain.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis='columns')\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis='columns')\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "dt_simple = DecisionTreeClassifier(min_samples_split=30) ### Make model much simplier by requiring 30 samples to split\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "dt_simple = dt_simple.fit(X_train, Y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Y_pred = dt_simple.predict(X_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print('Metrics of Model on Test Data')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_wet}\")\n",
    "print(f\"Recall: {recall_wet}\")\n",
    "print(f\"F1-Score: {F_wet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>ANSWER THE QUESTION BELOW.</font> Compare the results above with the results from the beginning of Part 2 (smalltrain.csv with the original decision tree without decreasing model complexity), what did you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before:\n",
      "\n",
      "Accuracy: 0.707\n",
      "Precision: 0.6628440366972477\n",
      "Recall: 0.664367816091954\n",
      "F1-Score: 0.6636050516647533\n",
      "\n",
      "After:\n",
      "Accuracy : 0.757\n",
      "Precision : 0.6882352941176471\n",
      "Recall : 0.8068965517241379\n",
      "F1-Score : 0.742857142857143\n",
      "\n",
      "We can see that the accuracy has increased, but more significantly the Recall has increased by 14.2% and the F1-Score \n",
      "has increased by 7.9%. This means that the model is now correctly predicting actual wetland cases better. Therefore,\n",
      "the decrease in model complexity has improved the model's generalization to the test data.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MY ANSWER IS xxxxx\n",
    "ans = '''\n",
    "Before:\n",
    "\n",
    "Accuracy: 0.707\n",
    "Precision: 0.6628440366972477\n",
    "Recall: 0.664367816091954\n",
    "F1-Score: 0.6636050516647533\n",
    "\n",
    "After:\n",
    "Accuracy : 0.757\n",
    "Precision : 0.6882352941176471\n",
    "Recall : 0.8068965517241379\n",
    "F1-Score : 0.742857142857143\n",
    "\n",
    "We can see that the accuracy has increased, but more significantly the Recall has increased by 14.2% and the F1-Score \n",
    "has increased by 7.9%. This means that the model is now correctly predicting actual wetland cases better. Therefore,\n",
    "the decrease in model complexity has improved the model's generalization to the test data.\n",
    "\n",
    "'''\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare different model on the same test data\n",
    "In this part, you will train other types of models and evaluate on the test data. You will compare their classification performance.\n",
    "\n",
    "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> PLEASE USE THE SAME TRAINING AND TEST DATA as in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   near infra red   red  green  blue  class\n",
      "0              123  132    115   133      0\n",
      "1              152  150    119   187      1\n",
      "2              169  166    143   192      1\n",
      "3               55   49     43    97      0\n",
      "4              141  135    117   181      1\n",
      "   near infra red   red  green  blue  class\n",
      "0              137  140    129   150      0\n",
      "1              169  162    140   193      1\n",
      "2              124  110     89   162      1\n",
      "3              105  104     99   153      1\n",
      "4              105  102     88   173      1\n",
      "Accuracy: 0.757\n",
      "Precision: 0.7307692307692307\n",
      "Recall: 0.6988505747126437\n",
      "F1-Score: 0.7144535840188013\n"
     ]
    }
   ],
   "source": [
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis='columns')\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis='columns')\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "# train a logistic regression model; e.g., use the LogisticRegression model in Scikit-Learn.\n",
    "# using parameter \"solver='liblinear'\"\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr = lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)\n",
    "\n",
    "# evalute the logistic regression model on test data\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_wet}\")\n",
    "print(f\"Recall: {recall_wet}\")\n",
    "print(f\"F1-Score: {F_wet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "Precision: 0.7406679764243614\n",
      "Recall: 0.8666666666666667\n",
      "F1-Score: 0.798728813559322\n"
     ]
    }
   ],
   "source": [
    "# train a Support Vector Machine (SVM) model, e.g., the SVC model in Scikit-Learn, choose parameters approprioately\n",
    "\n",
    "# Please find the SVC function in Scikit-Learn, use parameters \"gamma='scale', C=100\"\n",
    "clf = SVC(gamma='scale', C=100)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evalute the Support Vector Machine (SVM) model on test data\n",
    "Y_pred = clf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_wet}\")\n",
    "print(f\"Recall: {recall_wet}\")\n",
    "print(f\"F1-Score: {F_wet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>ANSWER THE QUESTION BELOW.</font> How do you compare the results from different models above with decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decison Tree ----------------\n",
      "Accuracy : 0.757\n",
      "Precision : 0.6882352941176471\n",
      "Recall : 0.8068965517241379\n",
      "F1-Score : 0.742857142857143\n",
      "\n",
      "Logistic Regression ---------\n",
      "Accuracy: 0.757\n",
      "Precision: 0.7307692307692307\n",
      "Recall: 0.6988505747126437\n",
      "F1-Score: 0.7144535840188013\n",
      "\n",
      "Support Vector Machine ------\n",
      "Accuracy: 0.81\n",
      "Precision: 0.7406679764243614\n",
      "Recall: 0.8666666666666667\n",
      "F1-Score: 0.798728813559322\n",
      "\n",
      "The decision tree performed better than the logistic regression model on the recall, but worse on the precision. This means\n",
      "that the decision tree was better at predicting actual wetland cases, but the logistic regression model was better at being \n",
      "correct on its predicted wetland cases. The SVM model performed the best out of the three models. It had the highest accuracy,\n",
      "precision, recall, and F1-Score. This means that the SVM model was the best at predicting actual wetland cases and being correct\n",
      "on its predicted wetland cases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans = '''\n",
    "Decison Tree ----------------\n",
    "Accuracy : 0.757\n",
    "Precision : 0.6882352941176471\n",
    "Recall : 0.8068965517241379\n",
    "F1-Score : 0.742857142857143\n",
    "\n",
    "Logistic Regression ---------\n",
    "Accuracy: 0.757\n",
    "Precision: 0.7307692307692307\n",
    "Recall: 0.6988505747126437\n",
    "F1-Score: 0.7144535840188013\n",
    "\n",
    "Support Vector Machine ------\n",
    "Accuracy: 0.81\n",
    "Precision: 0.7406679764243614\n",
    "Recall: 0.8666666666666667\n",
    "F1-Score: 0.798728813559322\n",
    "\n",
    "The decision tree performed better than the logistic regression model on the recall, but worse on the precision. This means\n",
    "that the decision tree was better at predicting actual wetland cases, but the logistic regression model was better at being \n",
    "correct on its predicted wetland cases. The SVM model performed the best out of the three models. It had the highest accuracy,\n",
    "precision, recall, and F1-Score. This means that the SVM model was the best at predicting actual wetland cases and being correct\n",
    "on its predicted wetland cases.\n",
    "'''\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ensemble learning\n",
    "In this part, you will run several ensemble learning of decision trees, including bagging and random forest. For random forest, you can directly call it as a separate model from library.\n",
    "\n",
    "<font color=red>PLEASE COMPLETE CODES BELOW</font>. PLEASE USE THE ORIGINAL TRAINING AND TEST DATA in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   near infra red   red  green  blue  class\n",
      "0              123  132    115   133      0\n",
      "1              152  150    119   187      1\n",
      "2              169  166    143   192      1\n",
      "3               55   49     43    97      0\n",
      "4              141  135    117   181      1\n",
      "   near infra red   red  green  blue  class\n",
      "0              137  140    129   150      0\n",
      "1              169  162    140   193      1\n",
      "2              124  110     89   162      1\n",
      "3              105  104     99   153      1\n",
      "4              105  102     88   173      1\n",
      "Accuracy: 0.804\n",
      "Precision: 0.7798594847775175\n",
      "Recall: 0.7655172413793103\n",
      "F1-Score: 0.7726218097447796\n"
     ]
    }
   ],
   "source": [
    "#note: class 0 for dry land, class 1 for wetland\n",
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis='columns')\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis='columns')\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "# please train bagging of decision tree, using BaggingClassifier with default parameters\n",
    "clf = BaggingClassifier()\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "# please evaluate it on the test data\n",
    "Y_pred = clf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_wet}\")\n",
    "print(f\"Recall: {recall_wet}\")\n",
    "print(f\"F1-Score: {F_wet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802\n",
      "Precision: 0.7736720554272517\n",
      "Recall: 0.7701149425287356\n",
      "F1-Score: 0.7718894009216589\n"
     ]
    }
   ],
   "source": [
    "# please train a random forest, using RandomForestClassifier function with paramters \"n_estimators=50\"\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "# please evaluate it on the test data\n",
    "Y_pred = clf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision_wet}\")\n",
    "print(f\"Recall: {recall_wet}\")\n",
    "print(f\"F1-Score: {F_wet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLEASE ANSWER THE QUESTION BELOW. How do you compare different ensemble methods? Which one has the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging Classifier ----------------\n",
      "Accuracy: 0.803\n",
      "Precision: 0.7754629629629629\n",
      "Recall: 0.7701149425287356\n",
      "F1-Score: 0.7727797001153403\n",
      "\n",
      "Random Forest Classifier ---------\n",
      "Accuracy: 0.813\n",
      "Precision: 0.7818181818181819\n",
      "Recall: 0.7908045977011494\n",
      "F1-Score: 0.7862857142857143\n",
      "\n",
      "The Random Forest Classifier performed better than the Bagging Classifier on all metrics. The fact that random forests also manipulate the input feature at each\n",
      "decision making node split. This means that the random forest model is more robust than the bagging model. The random forest model is also less likely to overfit\n",
      "than the bagging model. This is why the random forest model performed better than the bagging model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans = '''\n",
    "Bagging Classifier ----------------\n",
    "Accuracy: 0.803\n",
    "Precision: 0.7754629629629629\n",
    "Recall: 0.7701149425287356\n",
    "F1-Score: 0.7727797001153403\n",
    "\n",
    "Random Forest Classifier ---------\n",
    "Accuracy: 0.813\n",
    "Precision: 0.7818181818181819\n",
    "Recall: 0.7908045977011494\n",
    "F1-Score: 0.7862857142857143\n",
    "\n",
    "The Random Forest Classifier performed better than the Bagging Classifier on all metrics. The fact that random forests also manipulate the input feature at each\n",
    "decision making node split. This means that the random forest model is more robust than the bagging model. The random forest model is also less likely to overfit\n",
    "than the bagging model. This is why the random forest model performed better than the bagging model.\n",
    "'''\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (EXTRA CREDIT): Implement your own classifier (logistic regression)\n",
    "\n",
    "You should implement your logistic regression model: a training function and a test function. Save your source codes in 'myLR.py'. Put the python script in the same dirctory of this Jupyter Notebook. Load the scripts so that you can call your own training and prediction functions. Then evaluate the results on test data. Compare your results from the results from built-in logistic regression library in Part 4. Please use the same training data and test data as Part 1. Please make sure you print out the accuracy, confusion matrix, precision, recall, F-score. \n",
    "\n",
    "<b>Requirement</b>\n",
    "\n",
    "- Your codes should run through without bugs. Codes with bugs in Jupyter notebook running will NOT be graded. \n",
    "    \n",
    "- You CANNOT copy a same python codes from online for this question. It will be treated as cheating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NOT ATTEMPTED COMMENTED OUT TO NOT RUN ERRORS \\n# Load data\\ncol_names = [\\'near infra red \\', \\'red\\', \\'green\\', \\'blue\\', \\'class\\'] \\nfeatures =  [\\'near infra red \\', \\'red\\', \\'green\\', \\'blue\\'] \\n\\n# load dataset\\ntrain_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\\nprint(train_dat.head())\\n\\ntest_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\\nprint(test_dat.head())\\n\\n# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\\nX_train = train_dat.drop(\\'class\\', axis=\\'columns\\')\\nY_train = train_dat[\\'class\\']\\n\\n# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\\nX_test = test_dat.drop(\\'class\\', axis=\\'columns\\')\\nY_test = test_dat[\\'class\\']\\n\\n# Load your implemented codes myLR.py\\nfrom myLR import MyLR\\n\\n# Call your own function to train a logistic regression model, as what we did in Part 1 with decision tree\\nmyLR = MyLR()\\nmyLR = myLR.fit(X_train, Y_train)\\n\\n# Call your own function to make prediction on test data, and evaluate those metrics, as what we did in Part 1 with deicsion tree\\nY_pred = \\nprint(Y_pred.head())\\nprint(Y_pred.shape)\\n\\n# evalute the logistic regression model on test data\\nconfusion_matrix = \\naccuracy = \\nprecision_wet = \\nrecall_wet = \\nF_wet = \\n\\nprint(confusion_matrix)\\nprint(accuracy)\\nprint(precision_wet)\\nprint(recall_wet)\\nprint(F_wet)\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" NOT ATTEMPTED COMMENTED OUT TO NOT RUN ERRORS \n",
    "# Load data\n",
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis='columns')\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis='columns')\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "# Load your implemented codes myLR.py\n",
    "from myLR import MyLR\n",
    "\n",
    "# Call your own function to train a logistic regression model, as what we did in Part 1 with decision tree\n",
    "myLR = MyLR()\n",
    "myLR = myLR.fit(X_train, Y_train)\n",
    "\n",
    "# Call your own function to make prediction on test data, and evaluate those metrics, as what we did in Part 1 with deicsion tree\n",
    "Y_pred = \n",
    "print(Y_pred.head())\n",
    "print(Y_pred.shape)\n",
    "\n",
    "# evalute the logistic regression model on test data\n",
    "confusion_matrix = \n",
    "accuracy = \n",
    "precision_wet = \n",
    "recall_wet = \n",
    "F_wet = \n",
    "\n",
    "print(confusion_matrix)\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
